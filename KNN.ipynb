{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors (KNN)\n",
    "==============================\n",
    "\n",
    "* Basic Understanding (wiki pedia):\n",
    "-------------------------------\n",
    ">In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n",
    "      >1. In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "      >2. In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "      \n",
    ">k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "\n",
    "\n",
    "* Hyperparameters :\n",
    "---------------------------------\n",
    "+ k:\n",
    ">find the best k on the validation set\n",
    "+ distance:\n",
    ">L1 vs. L2. It is interesting to consider differences between the two metrics. In particular, the L2 distance is much more unforgiving than the L1 distance when it comes to differences between two vectors. That is, the L2 distance prefers many medium disagreements to one big one. L1 and L2 distances (or equivalently the L1/L2 norms of the differences between a pair of images) are the most commonly used special cases of a p-norm.\n",
    "\n",
    "\n",
    "* Pros and Cons\n",
    "----------------------------------\n",
    "+ Pros:\n",
    ">1. simple & takes no time to train \n",
    ">2. a good choice if the data is low-dimensional\n",
    "+ Cons:\n",
    ">1. large computional cost at test time \n",
    "\n",
    "\n",
    "* Improvement\n",
    "----------------------------------\n",
    "+ Approximate Nearest Nerghbor (ANN) (e.g. FLANN)\n",
    ">trade off the correctness of the nearest neighbor retrieval with its space/time complexity during retrieval,and usually rely on a pre-processing/indexing stage that involves building a kdtree, or running the k-means algorithm.\n",
    "\n",
    "\n",
    "* Tips:\n",
    "-----------------------------------\n",
    ">*Evaluate on the test set only a single time, at the very end.*\n",
    "\n",
    ">*Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance.*\n",
    "\n",
    "\n",
    "* Ref:\n",
    "-----------------------------------\n",
    "+ http://cs231n.github.io/classification/\n",
    "+ https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Algorithm\n",
    "+ https://zhuanlan.zhihu.com/p/21354489?refer=c_35627586\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNN():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        # X: nxd  y:nx1\n",
    "        self.Xtr=X\n",
    "        self.ytr=y\n",
    "    def predict(self,X,k):\n",
    "        num_test=X.shape[0]\n",
    "        Ypred=np.zeros(num_test,dtype=self.ytr.dtype)\n",
    "        \n",
    "        # loop over all test rows\n",
    "        for i in xrange(num_test):\n",
    "            # distance=np.sum(np.abs(self.Xtr-X[i,:]),axis=1) # L1\n",
    "            # distance=np.sqrt(np.sum(np.square(sef.Xtr-X[i,:]),axis=1)) # L2\n",
    "            distance=np.sum(np.square(sef.Xtr-X[i,:]),axis=1) # equal to L2\n",
    "            threshold=np.sort(distance)[k]\n",
    "            min_k_index=np.where(distance<threshold)\n",
    "            Ypred[i]=self.ytr[min_index] \n",
    "        \n",
    "        return Ypred\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
