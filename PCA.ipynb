{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Principal Components Analysis)\n",
    "## * 作用：降维 消除数据冗余 防止过拟合\n",
    "## * 数学定义（维基）:PCA的数学定义是：一个正交化线性变换，把数据变换到一个新的坐标系统中，使得这一数据的任何投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推.\n",
    "## * 步骤+原理：\n",
    "\n",
    "### 1. 均值 方差归一化 \n",
    "### 2. 计算协方差矩阵 \n",
    "    数据为nxd，n为实例个数，d维为特征维数，计算列与列之间的协方差矩阵即代表特征之间的相关性\n",
    "### 3. 计算协方差矩阵的特征值和特征向量 \n",
    "    弄清楚特征值与特征向量的概念，这里特征值越大，即代表协方差矩阵的在对应的特征向量上的投影值越大，即特征之间的方差越大，这里又可以从两个方面理解，a.方差越大即特征之间的相关性越小，特征值冗余度越小，故要保留 b. 噪声肯定是存在的，从下图可以看出数据在噪声方向的方差远远小于数据可分方向的方差，故保留特征值大的特征向量的方向 \n",
    "   ![示意图](http://deeplearning.stanford.edu/wiki/images/b/b4/PCA-u1.png)\n",
    "### 4. 将特征值从大到小排序，保留前k个特征值（自己指定降维数） 或者 根据保留的特征值之和/所有特征值和=信息保留来进行选择需保留的特征（一般至少大于90%）\n",
    "### 5. 将上一步选出的特征值所对应的特征向量组合起来,即为变换矩阵\n",
    "### 6. 将输入数据和变换矩阵相乘,即为降维后的数据\n",
    "\n",
    "## * tips:\n",
    "### 实际应用中是使用数据数据训练PCA得到第5步的变换矩阵保存下来，再对训练数据和测试数据都进行同样的降维变换（这样训练数据和测试数据是投影到同一个空间）\n",
    "## * references：\n",
    "http://deeplearning.stanford.edu/wiki/index.php/PCA  \n",
    "https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pca_train #\n",
    "# input\n",
    "# mat: (nxd) the data matrix we need to reduce dimention #\n",
    "# k : the feature dimention after pca, make sure k<=n #\n",
    "# output\n",
    "# mat_tf: (dxk) the transfer matrix #\n",
    "def pca_train(mat,k):\n",
    "    # step 1\n",
    "    mat_mean=np.mean(mat,axis=0) #get mean according to column\n",
    "    mat_std=np.std(mat) # get variance\n",
    "    mat=(mat-mat_mean)/mat_std # normalization\n",
    "    # step 2\n",
    "    mat_cov=np.cov(mat,rowvar=0) # dxd\n",
    "    # step 3\n",
    "    eigval,eigvec=np.linalg.eig(mat_cov)\n",
    "    # step 4\n",
    "    ind=np.argsort(-eigval,axis=0) # sort ergval\n",
    "    infor_ratio=sum(eigval[ind[0:k]])/sum(eigval) # we can print the information ratio, better ensure that the value is greater than 0.9\n",
    "    print 'information ratio: ',infor_ratio\n",
    "    # step 5\n",
    "    mat_tf=eigvec[:,ind[0:k]]  # dxk\n",
    "    #mat_final=np.dot(mat,mat_tf)\n",
    "    #mat_reco=mat_final*mat_tf.T+mat_mean\n",
    "    return mat_tf\n",
    "# pca_excute #\n",
    "# input\n",
    "# mat: (nxd) the data matrix we need to reduce dimention #\n",
    "# mat_tf: (dxk) the transfer matrix #\n",
    "# output\n",
    "# mat_result: (nxk) the reduced matrix #\n",
    "def pca_excute(mat,mat_tf):\n",
    "    # step 1\n",
    "    mat_mean=np.mean(mat,axis=0)\n",
    "    mat_std=np.std(mat)\n",
    "    mat=(mat-mat_mean)/mat_std\n",
    "    # step 6\n",
    "    return np.dot(mat,mat_tf) # nxd dxk ->nxk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500L, 256L)\n",
      "information ratio:  0.953246409878\n",
      "(500L, 200L)\n"
     ]
    }
   ],
   "source": [
    "# test #\n",
    "from numpy import random\n",
    "a= random.random(size=(500,256))\n",
    "print a.shape\n",
    "mat_tf=pca_train(a,200)\n",
    "b=pca_excute(a,mat_tf)\n",
    "print b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
